{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Inferentia inference on Amazon EC2 Inf1 instance\n",
    "This example demonstrates AWS Inferentia inference with TensorFlow and AWS Neuron SDK compiler and runtime\n",
    "\n",
    "This example was tested on Amazon EC2 `inf1.xlarge` the following AWS Deep Learning AMI: \n",
    "`Deep Learning AMI (Ubuntu 18.04) Version 35.0`\n",
    "\n",
    "Run this notebook using the following conda environment:\n",
    "`aws_neuron_tensorflow_p36`\n",
    "\n",
    "Prepare your imagenet validation TFRecord files using the following helper script:\n",
    "https://github.com/tensorflow/models/blob/archive/research/inception/inception/data/download_and_preprocess_imagenet.sh\n",
    "\n",
    "Save it to `/home/ubuntu/datasets/` or update the dataset location in the `get_dataset()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.neuron as tfn\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "from tensorflow.keras.applications import ( \n",
    "    xception,\n",
    "    vgg16,\n",
    "    vgg19,\n",
    "    resnet,\n",
    "    resnet50,\n",
    "    resnet_v2,\n",
    "    inception_v3,\n",
    "    inception_resnet_v2,\n",
    "    mobilenet,\n",
    "    densenet,\n",
    "    nasnet,\n",
    "    mobilenet_v2\n",
    ")\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# from concurrent import futures\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "models = {\n",
    "#     'xception':xception,\n",
    "    'vgg16':vgg16,\n",
    "#     'vgg19':vgg19,\n",
    "#     'resnet50':resnet50,\n",
    "#     'resnet101':resnet,\n",
    "#     'resnet152':resnet,\n",
    "#     'resnet50_v2':resnet_v2,\n",
    "#     'resnet101_v2':resnet_v2,\n",
    "#     'resnet152_v2':resnet_v2,\n",
    "#     'resnext50':resnext,\n",
    "#     'resnext101':resnext,\n",
    "#     'inception_v3':inception_v3,\n",
    "#     'inception_resnet_v2':inception_resnet_v2,\n",
    "#     'mobilenet':mobilenet,\n",
    "#     'densenet121':densenet,\n",
    "#     'densenet169':densenet,\n",
    "#     'densenet201':densenet,\n",
    "#     'nasnet':nasnet,\n",
    "#     'nasnet':nasnet,\n",
    "#     'mobilenet_v2':mobilenet_v2\n",
    "}\n",
    "\n",
    "models_detail = {\n",
    "#     'xception':xception.Xception(weights='imagenet'),\n",
    "    'vgg16':vgg16.VGG16(weights='imagenet'),\n",
    "#     'vgg19':vgg19.VGG19(weights='imagenet'),\n",
    "#     'resnet50':resnet50.ResNet50(weights='imagenet'),\n",
    "#     'resnet101':resnet.ResNet101(weights='imagenet'),\n",
    "#     'resnet152':resnet.ResNet152(weights='imagenet'),\n",
    "#     'resnet50_v2':resnet_v2.ResNet50V2(weights='imagenet'),\n",
    "#     'resnet101_v2':resnet_v2.ResNet101V2(weights='imagenet'),\n",
    "#     'resnet152_v2':resnet_v2.ResNet152V2(weights='imagenet'),\n",
    "#     'resnext50':resnext.ResNeXt50(weights='imagenet'),\n",
    "#     'resnext101':resnext.ResNeXt101(weights='imagenet'),\n",
    "#     'inception_v3':inception_v3.InceptionV3(weights='imagenet'),\n",
    "#     'inception_resnet_v2':inception_resnet_v2.InceptionResNetV2(weights='imagenet'),\n",
    "#     'mobilenet':mobilenet.MobileNet(weights='imagenet'),\n",
    "#     'densenet121':densenet.DenseNet121(weights='imagenet'),\n",
    "#     'densenet169':densenet.DenseNet169(weights='imagenet'),\n",
    "#     'densenet201':densenet.DenseNet201(weights='imagenet'),\n",
    "#     'nasnet':nasnet.NASNetLarge(weights='imagenet'),\n",
    "#     'nasnet':nasnet.NASNetMobile(weights='imagenet'),\n",
    "#     'mobilenet_v2':mobilenet_v2.MobileNetV2(weights='imagenet')\n",
    "}\n",
    "\n",
    "\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2\n"
     ]
    }
   ],
   "source": [
    "model_type = 'vgg16'\n",
    "\n",
    "# https://github.com/tensorflow/tensorflow/issues/29931\n",
    "temp = tf.zeros([8, 224, 224, 3])\n",
    "_ = models[model_type].preprocess_input(temp)\n",
    "\n",
    "print('test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50 FP32 saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: vgg16_saved_model/assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Export SavedModel\n",
    "\n",
    "saved_model_dir = f'{model_type}_saved_model'\n",
    "shutil.rmtree(saved_model_dir, ignore_errors=True)\n",
    "\n",
    "model = models_detail[model_type]\n",
    "\n",
    "model.save(saved_model_dir)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(saved_model_dir, compile=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile models with different batch sizes and cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_inf1_model(saved_model_dir, inf1_model_dir, batch_size=1, num_cores=1, use_static_weights=False):\n",
    "    print(f'-----------batch size: {batch_size}, num cores: {num_cores}----------')\n",
    "    print('Compiling...')\n",
    "    \n",
    "    compiled_model_dir = f'{model_type}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "    inf1_compiled_model_dir = os.path.join(inf1_model_dir, compiled_model_dir)\n",
    "    shutil.rmtree(inf1_compiled_model_dir, ignore_errors=True)\n",
    "\n",
    "    example_input = np.zeros([batch_size,224,224,3], dtype='float32')\n",
    "\n",
    "    compiler_args = ['--verbose','1', '--neuroncore-pipeline-cores', str(num_cores)]\n",
    "    if use_static_weights:\n",
    "        compiler_args.append('--static-weights')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    compiled_model = tfn.trace(model,example_input)\n",
    "    compiled_res = compiled_model.save(inf1_compiled_model_dir)\n",
    "    print(f'Compile time: {time.time() - start_time}')\n",
    "    \n",
    "    compile_success = False\n",
    "    perc_on_inf = compiled_res['OnNeuronRatio'] * 100\n",
    "    if perc_on_inf > 50:\n",
    "        compile_success = True\n",
    "            \n",
    "    print(inf1_compiled_model_dir)\n",
    "    print(compiled_res)\n",
    "    print('----------- Done! ----------- \\n')\n",
    "    \n",
    "    return compile_success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tf.data` to read ImageNet validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image_record(record):\n",
    "    feature_map = {'image/encoded': tf.io.FixedLenFeature([], tf.string, ''),\n",
    "                  'image/class/label': tf.io.FixedLenFeature([1], tf.int64, -1),\n",
    "                  'image/class/text': tf.io.FixedLenFeature([], tf.string, '')}\n",
    "    obj = tf.io.parse_single_example(serialized=record, features=feature_map)\n",
    "    imgdata = obj['image/encoded']\n",
    "    label = tf.cast(obj['image/class/label'], tf.int32)   \n",
    "    label_text = tf.cast(obj['image/class/text'], tf.string)   \n",
    "    return imgdata, label, label_text\n",
    "\n",
    "def val_preprocessing(record):\n",
    "    imgdata, label, label_text = deserialize_image_record(record)\n",
    "    label -= 1\n",
    "    image = tf.io.decode_jpeg(imgdata, channels=3, \n",
    "                              fancy_upscaling=False, \n",
    "                              dct_method='INTEGER_FAST')\n",
    "\n",
    "    shape = tf.shape(image)\n",
    "    height = tf.cast(shape[0], tf.float32)\n",
    "    width = tf.cast(shape[1], tf.float32)\n",
    "    side = tf.cast(tf.convert_to_tensor(256, dtype=tf.int32), tf.float32)\n",
    "\n",
    "    scale = tf.cond(tf.greater(height, width),\n",
    "                  lambda: side / width,\n",
    "                  lambda: side / height)\n",
    "    \n",
    "    new_height = tf.cast(tf.math.rint(height * scale), tf.int32)\n",
    "    new_width = tf.cast(tf.math.rint(width * scale), tf.int32)\n",
    "    \n",
    "    image = tf.image.resize(image, [new_height, new_width], method='bicubic')\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    \n",
    "    image = tf.keras.applications.mobilnet.preprocess_input(image)\n",
    "    \n",
    "    return image, label, label_text\n",
    "\n",
    "def get_dataset(batch_size, use_cache=False):\n",
    "    data_dir = '/home/ubuntu/datasets/*'\n",
    "    files = tf.io.gfile.glob(os.path.join(data_dir))\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    \n",
    "    dataset = dataset.map(map_func=val_preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(count=1)\n",
    "    \n",
    "    if use_cache:\n",
    "        shutil.rmtree('tfdatacache', ignore_errors=True)\n",
    "        os.mkdir('tfdatacache')\n",
    "        dataset = dataset.cache(f'./tfdatacache/imagenet_val')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single AWS Inferentia chip execution\n",
    "* Single core compiled models with automatic data parallel model upto 4 cores\n",
    "* Multi-core compiled models for pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inf1_predict_benchmark_single_threaded(neuron_saved_model_name, batch_size, user_batch_size, num_cores, use_cache=False, warm_up=10):\n",
    "    print(f'Running model {neuron_saved_model_name}, user_batch_size: {user_batch_size}\\n')\n",
    "\n",
    "    model_inf1 = tf.contrib.predictor.from_saved_model(neuron_saved_model_name)\n",
    "\n",
    "    iter_times = []\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    display_threshold = 0\n",
    "    warm_up = 10\n",
    "\n",
    "    ds = get_dataset(user_batch_size, use_cache)\n",
    "\n",
    "    ds_iter = ds.make_initializable_iterator()\n",
    "    ds_next = ds_iter.get_next()\n",
    "    ds_init_op = ds_iter.initializer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if use_cache:\n",
    "            sess.run(ds_init_op)\n",
    "            print('\\nCaching dataset ...')\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    (validation_ds,label,_) = sess.run(ds_next)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(f'Caching finished: {time.time()-start_time} sec')  \n",
    "\n",
    "        try:\n",
    "            sess.run(ds_init_op)\n",
    "            counter = 0\n",
    "            \n",
    "            total_datas = 1000\n",
    "            display_every = 100\n",
    "            display_threshold = display_every\n",
    "            \n",
    "            ipname = list(model_inf1.feed_tensors.keys())[0]\n",
    "            resname = list(model_inf1.fetch_tensors.keys())[0]\n",
    "            \n",
    "            walltime_start = time.time()\n",
    "            sess_time = []\n",
    "            extend_time = []\n",
    "            while True:\n",
    "                sess_start = time.time()\n",
    "                (validation_ds,batch_labels,_) = sess.run(ds_next)\n",
    "                sess_time.append(time.time() - sess_start)\n",
    "                \n",
    "                model_feed_dict={ipname: validation_ds}\n",
    "\n",
    "                if counter == 0:\n",
    "                    for i in range(warm_up):\n",
    "                        _ = model_inf1(model_feed_dict);                    \n",
    "\n",
    "                start_time = time.time()\n",
    "                inf1_results = model_inf1(model_feed_dict);\n",
    "                iter_times.append(time.time() - start_time)\n",
    "                \n",
    "                extend_start = time.time()\n",
    "                actual_labels.extend(label for label_list in batch_labels for label in label_list)\n",
    "                pred_labels.extend(list(np.argmax(inf1_results[resname], axis=1)))\n",
    "                extend_time.append(time.time() - extend_start)\n",
    "                \n",
    "                if counter*user_batch_size >= display_threshold:\n",
    "                    print(f'Images {counter*user_batch_size}/{total_datas}. Average i/s {np.mean(user_batch_size/np.array(iter_times[-display_every:]))}')\n",
    "                    display_threshold+=display_every\n",
    "\n",
    "                counter+=1\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "    labeling_start = time.time()\n",
    "    \n",
    "    acc_inf1 = np.sum(np.array(actual_labels) == np.array(pred_labels))/len(actual_labels)\n",
    "    iter_times = np.array(iter_times)\n",
    "    labeling_time = time.time() - labeling_start\n",
    "    \n",
    "    results = pd.DataFrame(columns = [f'inf1_compiled_batch_size_{batch_size}_compiled_cores_{num_cores}'])\n",
    "    results.loc['compiled_batch_size']     = [batch_size]\n",
    "    results.loc['user_batch_size']         = [user_batch_size]\n",
    "    results.loc['accuracy']                = [acc_inf1]\n",
    "    results.loc['prediction_time']         = [np.sum(iter_times)]\n",
    "    results.loc['sess_time']               = [np.sum(np.array(sess_time))]\n",
    "    results.loc['extend_time']             = [np.sum(np.array(extend_time))]\n",
    "    results.loc['labeling_time']           = [np.sum(np.array(labeling_time))]\n",
    "    results.loc['wall_time']               = [time.time() - walltime_start]\n",
    "    results.loc['images_per_sec_mean']     = [np.mean(user_batch_size / iter_times)]\n",
    "    results.loc['images_per_sec_std']      = [np.std(user_batch_size / iter_times, ddof=1)]\n",
    "    results.loc['latency_mean']            = [np.mean(iter_times) * 1000]\n",
    "    results.loc['latency_99th_percentile'] = [np.percentile(iter_times, q=99, interpolation=\"lower\") * 1000]\n",
    "    results.loc['latency_median']          = [np.median(iter_times) * 1000]\n",
    "    results.loc['latency_min']             = [np.min(iter_times) * 1000]\n",
    "    display(results.T)\n",
    "    shutil.rmtree(neuron_saved_model_name, ignore_errors=True)\n",
    "\n",
    "    return results, iter_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1 core nums 1 compile start\n",
      "-----------batch size: 1, num cores: 1----------\n",
      "Compiling...\n",
      "WARNING:tensorflow:running a fall-back code generator to mitigate compilation failure\n",
      "INFO:tensorflow:Assets written to: vgg16_inf1_saved_models/vgg16_batch_1_inf1_cores_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg16_inf1_saved_models/vgg16_batch_1_inf1_cores_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile time: 311.6127519607544\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5db3acab7b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_of_cores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'core nums'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'compile start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcompile_inf1_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf1_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bebc50c687ee>\u001b[0m in \u001b[0;36mcompile_inf1_model\u001b[0;34m(saved_model_dir, inf1_model_dir, batch_size, num_cores, use_static_weights)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcompile_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mperc_on_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OnNeuronRatio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mperc_on_inf\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcompile_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "inf1_model_dir = f'{model_type}_inf1_saved_models'\n",
    "saved_model_dir = f'{model_type}_saved_model'\n",
    "\n",
    "\n",
    "# testing batch size\n",
    "batch_list = [1,2,4,8,16,32,64]\n",
    "num_of_cores = [1]\n",
    "for batch in batch_list:\n",
    "    for core in num_of_cores:\n",
    "        print('batch size:', batch,'core nums', core,'compile start')\n",
    "        compile_inf1_model(saved_model_dir, inf1_model_dir, batch_size=batch, num_cores=core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_tensorflow_p36",
   "language": "python",
   "name": "aws_neuron_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
